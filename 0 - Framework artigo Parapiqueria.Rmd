---
title: "Framework - Artigo Parapiqueria"
author:
  name: Gabriel Santos 
date: 
params: 
  update_dat:  !r paste(format(Sys.time(), '%Y-%m-%d (%H:%M:%S)'))
  orig_dat: "05/08/2024"
output: 
  html_notebook: 
    theme: paper
    code_folding: show
    toc: true
    toc_float:
      collapsed: true
    cache: true
    number_sections: false
editor_options: 
  markdown: 
    wrap: 72
---

**Criado em**: `r params$orig_dat`

**Atualizado:** `r params$update_dat`

<!-- Setup -->

```{css, echo=FALSE}
.scroll {
  max-height: 600px;
  overflow-y: auto;
  background-color: inherit;
}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, digits=3) 
options(knitr.table.format = "html")

```

<!-- Start here -->

```{r eval=FALSE, include=FALSE}
rm(list=ls());gc()
```

# README

<!--#  Reorganizar essa ordem e atualizar os novos tópicos adicionados: elasticidade e LTRE-->

1.  Creates an accessory function to automatize the implementation of the Quadratic Programming inverse method (QP therefore)

2.  Estimate survival, stasis and growth using the `QPmat`from `popbio` package

    1.  Step 1. Define the rules/constraints that must be given to the QP - Quadratic programming

        -   $C$, $b$ , non-zeros
        -   Automatise this process with a dedicated ancillary function `WoodPar`

3.  Reach a best method to estimate recruitment, as it cannot be directly estimated with QP

4.  Apply QP and retrieve MPMs

5.  Complement MPMs with recruitment

6.  Calculate lambda and plot results

7.  Population viability

    7.1. Simulations with reducing vital rates'

# 1. Packages and SessionInfo

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(popbio)
library(ggplot2)
library(tidyverse)
library(openxlsx)
library(ggrepel)
library(ggridges)
library(broom)

set.seed(1)
sessionInfo()

```

# 2. Data Loading and standardization

```{r}
Mar22<-read.xlsx("Dados brutos/Dados parapiqueria - Completo - Consolidado 05Aug2024.xlsx",sheet="Março2022")
Abr22<-read.xlsx("Dados brutos/Dados parapiqueria - Completo - Consolidado 05Aug2024.xlsx",sheet="Abril2022")
Mai22<-read.xlsx("Dados brutos/Dados parapiqueria - Completo - Consolidado 05Aug2024.xlsx",sheet="Maio2022")

### 2023
Mar23<-read.xlsx("Dados brutos/Dados parapiqueria - Completo - Consolidado 05Aug2024.xlsx",sheet="Março2023")
Abr23<-read.xlsx("Dados brutos/Dados parapiqueria - Completo - Consolidado 05Aug2024.xlsx",sheet="Abril2023")
Mai23<-read.xlsx("Dados brutos/Dados parapiqueria - Completo - Consolidado 05Aug2024.xlsx",sheet="Maio2023")

### 2024
Mar24<-read.xlsx("Dados brutos/Dados parapiqueria - Completo - Consolidado 05Aug2024.xlsx",sheet="Março2024")
Abr24<-read.xlsx("Dados brutos/Dados parapiqueria - Completo - Consolidado 05Aug2024.xlsx",sheet="Abril2024")
Mai24<-read.xlsx("Dados brutos/Dados parapiqueria - Completo - Consolidado 05Aug2024.xlsx",sheet="Maio2024")

```

## 2.1. Prepare final datasets: Merging censuses & summarise

-   Step 1: Merge monthly censuses –\> Merge census across the years

    Censuses are performed every March, April and May and need to merged to produce a timeserie.

    -   Two datasets were produced

    ```         
    -   A timeserie (Mar-May) to run QPmat

    -   A maximum number of individuals each year to estimate recruitment

        -   Censuses from different years must be paired
    ```

## 2.2. Merge censuses

> Quadrants are not necessary for the analyses

```{r message=FALSE, warning=FALSE}

census2024<-rbind(
  data.frame(Mar24,Month="3"),
  data.frame(Abr24,Month="4"),
  data.frame(Mai24,Month="5"))%>%
  as_tibble()%>%
  group_by(Site,Plot,Month)%>%    #Quadrants not included
  summarise(Imaturos=sum(Imaturos),
            Reprodutivos=sum(Reprodutivos))%>%
  ungroup()


### 2023
census2023<-rbind(
  data.frame(Mar23,Month="3"),
  data.frame(Abr23,Month="4"),
  data.frame(Mai23,Month="5"))%>%
  as_tibble()%>%
  group_by(Site,Plot,Month)%>%
  summarise(Imaturos=sum(Imaturos),
            Reprodutivos=sum(Reprodutivos))%>%
  ungroup()

### 2022
census2022<-rbind(
  data.frame(Mar22,Month="3"),
  data.frame(Abr22,Month="4"),
  data.frame(Mai22,Month="5"))%>%
  select(Site,Plot,Imaturos,Reprodutivos,Month)

census2022 # An example of a census merged along the year

#----------------------------------------------------
# Now merge the census across the years
#----------------------------------------------------

Census_all<-rbind(
  cbind(census2024,year="2024"),
  cbind(census2023,year="2023"),
  cbind(census2022,year="2022"))%>%
  pivot_longer(Imaturos:Reprodutivos,names_to="stage")


#### Maximum number of individuals
Census_all_max<-rbind(
  cbind(census2024,year="2024"),
  cbind(census2023,year="2023"),
  cbind(census2022,year="2022"))%>%
  group_by(Site,year,Plot)%>%
  summarise(MaxIm=max(Imaturos),
            MaxRep=max(Reprodutivos))%>%
  mutate(MaxTot=ifelse(MaxIm>MaxRep,MaxIm,MaxRep))%>%
  pivot_longer(MaxIm:MaxTot,names_to="stage")


Census_all #View

Census_all_max  #View
```

### 2.3. Pairing censuses to estimate recruitment

```{r message=FALSE, warning=FALSE}
Timelags_base<-Census_all_max %>%
  mutate(year = as.numeric(year)) %>%
  group_by(Plot) %>%
  arrange(year, stage) %>%
  mutate(VAR = paste0(stage, ">", 
                      lead(stage, order_by = interaction(year,stage)))) %>%
# Armazena o ano inicial e o ano seguinte para o cálculo do intervalo de tempo
  mutate(t0y = year,
         t1y = lead(year, order_by = VAR))%>%   #lead acessa o próximo valor, logo o ano t+1 já que os valores estão em ordem
  mutate(t0 = value, 
         t1 = lead(value, order_by = VAR)) %>% 
  # Filtra apenas os casos onde o ano seguinte (t1y) é maior que o ano inicial (t0y),
  filter(t1y>t0y)%>%	#It only returns with timelag. Change interaction to return same year variables
  arrange(Plot,VAR)

#### Max Rep in t0 vs. MaxTot in t1
#Pairing Maximum reprodutctive to Max Total

RepTot<-left_join(
  filter(Timelags_base,stage=="MaxRep")%>%select(Site:Plot,t0y,t0),
  filter(Timelags_base,stage=="MaxTot")%>%select(Site:Plot,t1y,t1))%>%
  mutate(VAR="MaxRep>MaxTot")%>%
  select(VAR,Site:t0y,t1y,t0,t1)%>%	#Reorder columns
  mutate(Site=as.factor(Site))%>%
  mutate(period=as.factor(paste0(t0y,"-",t1y)))

##### Max Rep in t0 vs. MaxIm in t1

RepIm<-left_join(
  filter(Timelags_base,stage=="MaxRep")%>%select(Site:Plot,t0y,t0),
  filter(Timelags_base,stage=="MaxIm")%>%select(Site:Plot,t1y,t1))%>%
  mutate(VAR="MaxRep>MaxIm")%>%
  select(VAR,Site:t0y,t1y,t0,t1)%>%	#Reorder columns
  mutate(Site=as.factor(Site))%>%
  mutate(period=as.factor(paste0(t0y,"-",t1y)))

RepTot # Exemplo de resultado
```

### 2.4. Transform censuses in timeseries to estimate survival

```{r}
## 2024
Census2024_ts<-census2024%>%arrange(Site,Plot,Month)%>%
select(Plot,Month,Imaturos,Reprodutivos)%>%
pivot_longer(c(Imaturos,Reprodutivos),names_to="Stage",values_to="Indv")%>%
pivot_wider(names_from=Month,values_from="Indv")%>%
arrange(Plot)%>%
mutate(Plot_tag=paste0("Plot_",Plot))%>%
select(-c(Stage,Plot))%>%
 group_split(Plot_tag, .keep = TRUE) %>%
  `names<-`({.} %>% map(~ .x$Plot_tag[1]) %>% unlist()) %>%
  ## If you want to discard the grouping variable, do the following step as well
  map(~ .x %>% select(-Plot_tag))

#Census2024_ts

## 2023
Census2023_ts<-census2023%>%arrange(Site,Plot,Month)%>%
select(Plot,Month,Imaturos,Reprodutivos)%>%
pivot_longer(c(Imaturos,Reprodutivos),names_to="Stage",values_to="Indv")%>%
pivot_wider(names_from=Month,values_from="Indv")%>%
arrange(Plot)%>%
mutate(Plot_tag=paste0("Plot_",Plot))%>%
select(-c(Stage,Plot))%>%
 group_split(Plot_tag, .keep = TRUE) %>%
  `names<-`({.} %>% map(~ .x$Plot_tag[1]) %>% unlist()) %>%
  ## If you want to discard the grouping variable, do the following step as well
  map(~ .x %>% select(-Plot_tag))

#Census2023_ts


## 2022
Census2022_ts<-census2022%>%arrange(Site,Plot,Month)%>%
select(Plot,Month,Imaturos,Reprodutivos)%>%
pivot_longer(c(Imaturos,Reprodutivos),names_to="Stage",values_to="Indv")%>%
pivot_wider(names_from=Month,values_from="Indv")%>%
arrange(Plot)%>%
mutate(Plot_tag=paste0("Plot_",Plot))%>%
select(-c(Stage,Plot))%>%
 group_split(Plot_tag, .keep = TRUE) %>%
  `names<-`({.} %>% map(~ .x$Plot_tag[1]) %>% unlist()) %>%
  ## If you want to discard the grouping variable, do the following step as well
  map(~ .x %>% select(-Plot_tag))



# Giving names to all plots to trace back

names(Census2022_ts)<-paste0(names(Census2022_ts),"_2022")
names(Census2023_ts)<-paste0(names(Census2023_ts),"_2023")
names(Census2024_ts)<-paste0(names(Census2024_ts),"_2024")


# Merge all timeseries
all_census_ts<-as.list(c(
		Census2022_ts,
		Census2023_ts,
		Census2024_ts))


# Check
## A timeserie from March to May
all_census_ts[1]


# Show censuses
lapply(all_census_ts,cbind,Stages=c("Juveniles","Adults"))%>%
    Map(cbind, Plot_year = names(.),.)%>%do.call(rbind,.)%>%
    setNames(c("Plot_year","Mar", "Apr", "May","Stage"))%>%relocate(Stage, .before = "Mar")

```

# 3. Recruitment analyses

## 3.1. Packages

```{r message=FALSE, warning=FALSE}
library(lme4)
library(DHARMa)
library(performance)
library(glmmTMB)
library(MuMIn)

options(na.action=na.fail) #Necessary for correct use of MuMIn
```

## 3.2. Estimating recruitment based on the maximum reproductive individuals

Assessing if the maximum number of immature individuals in $t_1$ can be predicted by the maximum number of of reproductive in the year before $t_0$

Negative binomial has the best fit

```{html eval=FALSE, include=FALSE}
<!--
###  Poisson model worst performed and several warning message
pois_RepIm<-glmmTMB::glmmTMB(t1~0+t0*Site+(1|period), 
      family=poisson(link = "log"),data=RepIm)

dredge(pois_RepIm)

# RESULT
  dsp((Int)) cnd(Sit)  cnd(t0) cnd(Sit:t0) df   logLik   AICc  delta weight
4          +        + 0.005436              4 -642.156 1293.5   0.00  0.663
8          +        + 0.004913           +  5 -641.522 1294.8   1.35  0.337
2          +        +                       3 -714.134 1434.9 141.48  0.000
3          +          0.004766              2 -750.634 1505.6 212.14  0.000
-->
```

```{r message=FALSE, warning=FALSE, cache=TRUE}
nb_RepIm<-glmmTMB::glmmTMB(t1~0+t0*Site+(1|period),
                           family=nbinom2(link = "log"),data=RepIm)


nb_RepIm_ls<-dredge(nb_RepIm)%>%get.models(subset=NA)

nb_RepIm_df<-data.frame(
  formula=format(cbind(lapply(nb_RepIm_ls,formula))),
  Var="Rep>Tot",Dist="Neg.Bin",
  AICcmodavg::aictab(nb_RepIm_ls))

nb_RepIm_df
```

```{r message=FALSE, warning=FALSE}
# Best model is t1~0+t0+Site+(1|census) Negative binomial
BestModel<-update(nb_RepIm,t1~0+t0+Site+(1|period))
BestModel%>%parameters::parameters(.,exponentiate =F)
```

Check differences between streams

```{html eval=FALSE, include=FALSE, code_folding="hide"}
<!-- TO THINK ABOUT! 
QUAL O MELHOR MODELO PARA ESTIMAR SE HÁ OU NÃO DIFERENÇA NO RECRUTAMENTO DE PARAPIQUERIA?

BestModel Força o intercepto em zero porém com isso o modelo para de comparar diferenças entre sítios. 

# Modelo
glmmTMB::glmmTMB(t1~t0+Site+(1|period),       # Veja que o intercepto foi incluido (removi "0 + " da equação)
                  family=nbinom2(link = "log"),data=RepIm)%>%summary()

# RESULTADO
             Estimate Std. Error z value Pr(>|z|)    
(Intercept)  3.939961   0.273631  14.399   <2e-16 ***
t0           0.007173   0.003201   2.241   0.0251 *  
SiteS11B    -0.575314   0.229811  -2.503   0.0123 *         
# Esse resultado indica claramente que há diferenças entre os riachos!

-->
```

```{r}
car::Anova(BestModel)
```

```{html eval=FALSE, include=FALSE, code_folding="hide"}
<!-- IMPORTANTE!
Uma coisa que me gerou muita insegurança no passado e ainda é uma questão pra mim é se eu deveria ou não incluir o intercepto nesses modelos. 

Uma análise mais atenciosa no entanto revela que essa não deveria ser uma questão. 
# 
O modelo por exemplo
bestS11B(t1 ~ 0 + t0 + (1|period)) remove o intercepto e estima o efeito de beta t0 em 0.009382 
Já com a remoção do intercepto o valor passa a ser 0.009047. 

Quando convertido para o exponencial (para voltar a únidade de medida de individuos) temos então 1.009426 e 1.009088 respectivamente. Uma diferença de 0.000338, que PROVAVELMENTE é irrelevante para o modelo.


De forma similar temos 


beta t0 = 0.007173 em BestModel (modelo global que compara entre sítios e remove o intercepto)
e  temos exatamente o mesmo valor (beta t0 =  0.007173) para o modelo incluindo o intercepto!

Eles são o mesmo? Sim porém ao incluir o intercept somos também capazes de ter uma estimativa direta da diferença entre os modelos
             Estimate Std. Error z value Pr(>|z|)    
(Intercept)  3.939961   0.273631  14.399   <2e-16 ***
t0           0.007173   0.003201   2.241   0.0251 *  
SiteS11B    -0.575314   0.229811  -2.503   0.0123 *       # Fica claro que os recrutamentos são diferentes!!!

enquanto que ao remover o intercepto temos 
Conditional model:
         Estimate Std. Error z value Pr(>|z|)    
t0       0.007173   0.003201   2.241   0.0251 *  
SiteS11C 3.939963   0.273631  14.399   <2e-16 ***
SiteS11B 3.364650   0.297025  11.328   <2e-16 ***

O modelo apesar de parecer apresentar resultados diferentes não o faz. Pq se diminuiirmos beta SiteS11C e beta SiteS11B temos exatamente a diferença de Beta siteS11B calculada quando temos o intercepto

SiteS11B-SiteS11C =  3.364650 - 3.939963 = -0.575314

-->
```

```{r eval=FALSE, include=FALSE}

ggeffects::predict_response(
  BestModel,terms=c("t0[0:208]", #[0,208] represent the minimum and maximum individuals record in plot per month and pear stage(?)
          "Site","period"))%>%plot()

```

```{r}
# Apply the same structure of the best model but for each stream
bestS11B<-update(BestModel,~0+t0+(1|period), data=filter(RepIm,Site=="S11B"))
bestS11C<-update(BestModel,~0+t0+(1|period), data=filter(RepIm,Site=="S11C"))
```

# 4. Inverse model implementation

## 4.1. Accessory function - `WoodPar`

Automatically detects C, b, and non-zero elements

Function parameters include:

-   ven is the target MPM

-   myC2ven make matrix $C$

    -   myC2ven must be make in two parts because it is not a simple identity matrix

-   mynonzero.ven determines the non-zero elements

-   $b$ elements <!--# Preciso melhorar a explicação-->

    <!--# BUG: Função ainda em desenvolvimento, parece não funcionar para matrizes 2x2 ainda. Não sei pq isso ocorre! -->

```{r}
WoodPar<-function(ven){
  mynonzero.ven<- which(ven> 0)	#Determina quem non-zero elementos da matriz de transição (subdiagonal + classes que reproduzem)
  myC2ven.part1<-diag(-1,length(mynonzero.ven))
  myC2ven.part2<-(numeric(dim(ven)[1]))%*%t(numeric(length(mynonzero.ven)))		#MODIFICADO!
  diag(myC2ven.part2[,-c(seq(from=1,to=ncol(myC2ven.part2),by=1))])<-1			#+RECENTE
  diag(myC2ven.part2[,-c(seq(from=0,to=(ncol(myC2ven.part2)),by=1))])<-1			#+RECENTE
  myC2ven.part2<-as.data.frame(myC2ven.part2)
  venR<-replacing(ven)
  colnames(myC2ven.part2)<-venR[mynonzero.ven]
  myC2ven.part2<-replace(myC2ven.part2, which(colnames(myC2ven.part2)=="F"),numeric(dim(myC2ven.part2)[[1]]))
  myC2ven.part2<-as.matrix(myC2ven.part2)
  colnames(myC2ven.part2)<-NULL
  myC2ven<-rbind(myC2ven.part1,myC2ven.part2)
  myb.ven<- apply(myC2ven, 1, max)
  out<-list(myC2ven,myb.ven,mynonzero.ven)
  names(out)<-c("C","b","nonzero")
  return(out)
}
```

## 4.2. Quadratic programming efficiency

-   Simulate an hypothetical population to assess model's accuracy
-   **Must run in order to determine C, non-zero elements and b which will be used to estimate survival**
    -   Hypothetical population was created based on the matrix structure we expect for *Parapiqueria*.
    -   Matrix is
        -   2x2
        -   divided in juveniles and adults
        -   Does note includes recruitment because it cannot be estimated within year (March-May) <!--# Isso é bem importante pq é aqui que está o segredo do nosso modelo -->
-   Also, contain model validation

```{r include=FALSE, echo=TRUE}
file.edit("ModelToyv1.R")
source("ModelToyv1.R")
```

## 4.3. Survival related demographic parameters estimation

### 4.3.1. Creating the constraint matrix for *Parapiqueria cavalcantei*

```{r}
newC<-WoodPar(A2)$C          
newC<-as.vector(newC)        
newC[c(4,9,15)]<-1           
newC<-newC%>%matrix(5)
newC
```

#### Define non-zero elements

```{r}
newnonzero<-WoodPar(A2)$nonzero
newnonzero #Non zero
```

```{r}
#b need adjustments
newb<-WoodPar(A2)$b
newb[c(1,2,3,4,5)]<-1
newb
```

# 5. Applying the quadratic programming

```{r}
mpm_list_bkp<-mpm_list<-NULL
for(i in 1:length(all_census_ts)){
mpm_list_bkp[[i]]<-mpm_list[[i]]<-tryCatch(
		QPmat(all_census_ts[[i]]%>%as.matrix(), 
			newC,
			newb,
			newnonzero),
 error = function(e) { return(NA)})
}

names(mpm_list)<-names(mpm_list_bkp)<-names(all_census_ts)

mpm_list[1]
```

## 5.1. Remove plots where vital rates could not be estimated

-   Two plots at S11B could not have demographic information estimated because individuals are close to zero.
-   Plots are:
-   Plot_19_2022 - Only adult individuals recorded?
-   Plot_15_2024 - Only 2 individuals in March

##### Decisions

-   Plot_15_2024 was below a extinction threshold, so, it is necessary to be included in the analyses.
-   Plot_19_2022 we cannot decided if it is a sampling error or a biological process so it was removed

```{r}
# Detect plots with problems
problems_plots<-mpm_list_bkp[lapply(lapply(mpm_list_bkp,complete.cases),any,FALSE)=="FALSE"]

problems_plots

all_census_ts[names(problems_plots)]

# Create a list with a single MPM with all elements equal to zero
zero_mpm<-list()
zero_mpm[[1]]<-matrix(rep(0,4),2)

mpm_list_bkp[names(mpm_list_bkp) %in% names(problems_plots)]
mpm_list_bkp[names(mpm_list_bkp) == "Plot_15_2024"]<-zero_mpm  #Include zero matrices in the list of estiamted matrices
mpm_list_bkp[names(mpm_list_bkp) %in% names(problems_plots)]


#Census2022_ts$Plot_15_2022 #Show trajectory of the abundance
#Census2023_ts$Plot_15_2023
#Census2024_ts$Plot_15_2024

# Now make mpm_list being equal to mpm_list_bkp
mpm_list <- mpm_list_bkp[!names(mpm_list)=="Plot_19_2022"]
```

# 6. Merging recruitment with survival dependent vital rates

## 6.1. Separe MPMs by streams

Separe plots according to the streams. - Plots 1-10 are placed in the S11C; - Plots 11-20 are placed in the S11B;

```{r}
mpm_list_S11C<-mpm_list[grep("^Plot_([1-9]|10)_.*$", names(mpm_list))]
mpm_list_S11B<-mpm_list[grep("^Plot_(1[1-9]|20)_.*$", names(mpm_list))]
```

## 6.2. Build MPM $A$ for each stream

### 6.2.1. Transform Recruitment in matrix $F$

Each stream has a proper mean $F$ matrix

```{r}
mean_matF_S11B<-matrix(
	c(0,0,
	exp(fixef(bestS11B)$cond[[1]]),
	  0),ncol=2)

mean_matF_S11C<-matrix(
	c(0,0,
	exp(fixef(bestS11C)$cond[[1]]),
	  0),ncol=2)


#Show matrix F
list(mean_matF_S11B, mean_matF_S11C)
```

### 6.2.2. Merging components of MPM $A = U + F$

```{r}
mpm_list_recruit_S11C<-Map(`+`, mpm_list_S11C, list(mean_matF_S11C))
mpm_list_recruit_S11B<-Map(`+`, mpm_list_S11B, list(mean_matF_S11B))

# Merge in a single list for statistics
mpm_list_recruit_full<-c(mpm_list_recruit_S11C,mpm_list_recruit_S11B)

# Example
mpm_list_recruit_full[1]
```

# 7. Summary results

An ancillary function to optimise statistical report

```{r}
statrep<- function(data, group_vars, value_var = "value") {
  data %>%
    group_by(across(all_of(group_vars))) %>%
    summarise(
      Mean = mean(.data[[value_var]], na.rm = TRUE),
      SD = sd(.data[[value_var]], na.rm = TRUE),
      N = n(),
      SE = SD / sqrt(N),
      CI = 1.96 * SE,
      lower95 = quantile(.data[[value_var]], 0.025, na.rm = TRUE),
      higher95 = quantile(.data[[value_var]], 0.975, na.rm = TRUE),
      .groups = "drop"
    ) %>%
    mutate(across(where(is.numeric), round, 2))
}
```

Transform vital rates into a data.frame to proceed with statistical analyses

```{r}
vr.df<-data.frame(do.call(rbind,lapply(mpm_list_recruit_full,as.vector)),var="vr")%>%rownames_to_column(var = "VAR")%>%
as_tibble()%>%
  setNames(c("VAR","S1","S2","R","S4","var"))%>%
separate(VAR,sep="_",into = c("x", "Plot", "year"))%>%
select(-x)%>%
pivot_longer(S1:S4,values_to="value",names_to="vital_rate")%>%
mutate(Plot=as.numeric(Plot),
	 year=year)%>%
mutate(Site=ifelse(Plot<11,"S11C","S11B"))
```

## 7.1. Overall Vital rates

```{r}
vr.df%>%
filter(vital_rate!="R")%>%
statrep(., group_vars = c("vital_rate"),value_var = "value")
```

## 7.2. Vital rates by plot

```{r}
vr.df%>%
statrep(., group_vars = c("vital_rate", "Site"),value_var = "value")%>%
filter(vital_rate!="R")%>%
    group_by(Site)%>% group_split()
```

### 7.2.1. Checking if there is differences between plots

```{r}
vr.df%>%
  filter(vital_rate!="R")%>%        # Recruitment need to be removed as it is a mean deterministic value, without variation. Note that difference in recruit was alread tested in the Recruitment section above
  select(-c(year,var))%>%
  nest(data=c(Plot,Site,value))%>%
  mutate(pairwise= map(data, ~ t.test(value ~ Site, data = .)))%>%
  pull(pairwise)%>%
  purrr::set_names(c("Stasis[Immat]", "Growth[Immat.>Adult]", "Surv[Adult]"))%>%
  tibble(
    Comparison = names(.),
    t_value = map_dbl(., ~ .x$statistic),
    df = map_dbl(., ~ .x$parameter),
    p_value = map_dbl(., ~ .x$p.value),
    conf_low = map_dbl(., ~ .x$conf.int[1]),
    conf_high = map_dbl(., ~ .x$conf.int[2]),
    mean_S11B = map_dbl(., ~ .x$estimate[1]),
    mean_S11C = map_dbl(., ~ .x$estimate[2])
  )

```

### 7.2.2. Checking if vital rates are correlated to year

```{r}
# Similar to above. Main difference is in the nest process
vr.df%>%
  filter(vital_rate!="R")%>%
  select(-c(Plot,Site,var))%>%
  mutate(year=as.numeric(year))%>%
  nest(data=c(year,value))%>% #only year and value are considered here
  mutate(pairwise= map(data, ~ cor.test(.$value, .$year,method="pearson")))%>% #cor.test instead of t.test
  pull()%>%
  purrr::set_names(c("Stasis[Immat]", "Growth[Immat.>Adult]", "Surv[Adult]"))%>%
  tibble(
    Vital_rate = names(.),
    Correlation = map_dbl(., ~ .x$estimate[1]),
    conf_low = map_dbl(., ~ .x$conf.int[1]),
    conf_high = map_dbl(., ~ .x$conf.int[2]),
    t = map_dbl(., ~ .x$statistic),
    df = map_dbl(., ~ .x$parameter),
    p_value = map_dbl(., ~ .x$p.value))

```

```{r}
vr.df%>%
  statrep(., group_vars = c("vital_rate","year","Site"))%>%
# filter(vital_rate!="R")%>%
  mutate(VR="Survival")%>%
mutate(stages=case_when(vital_rate=="S1"~ "Stasis[Immat]",
				vital_rate=="S2"~ "Growth[Immat.>Adult]",
				vital_rate=="R"~ "Recruitment",
				vital_rate=="S4"~ "Surv[Adult]"))%>%
mutate(stages=forcats::fct_relevel(stages, c("Statis[Immat]","Growth[Immat.>Adult]", "Surv[Adult]","Recruitment")))%>%
filter(vital_rate!="R")%>%
ggplot(.,aes(y=Mean,x=stages,group=year))+
geom_pointrange(aes(ymin=Mean-SE,ymax=Mean+SE,color=Site,shape=year),alpha=0.8,
	position = position_dodge2(width = 0.4))+
scale_x_discrete(labels=function(l) parse(text=l))+
scale_color_manual(values=c("#ff9211","#6e86e7"))+
theme_bw(base_size=16)+
theme(legend.position="bottom")+
facet_grid(.~VR)+
xlab(NULL)+ylab(NULL)
```

```{r}
lambsSite<-rbind(
do.call(rbind,lapply(mpm_list_recruit_S11C,lambda))%>%
	data.frame(Site="S11C",mod="model")%>%
			rownames_to_column(var = "VAR"),
do.call(rbind,lapply(mpm_list_recruit_S11B,lambda))%>%
	data.frame(Site="S11B",mod="model")%>%
			rownames_to_column(var = "VAR"))

#TESTING DIFFERENCES
t.test(lambsSite$.~lambsSite$Site)

```

```{r}
lambsSite_df<-lambsSite%>%rename(., Lambda=.)%>%
as_tibble()%>%
separate(VAR,sep="_",into = c("x", "Plot", "year"))%>%
select(-x)%>%
mutate(Plot=as.numeric(Plot),
	 year=year)%>%
mutate(Site=ifelse(Plot<11,"S11C","S11B"))%>%
statrep(., group_vars = c("Site", "year"),value_var = "Lambda")

lambsSite_df
```

```{r include=FALSE}
lambsPlot<-lambsSite_df%>%
mutate(VAR="PopGrowthRate")%>%
ggplot(.,aes(y=Mean,x=VAR,group=year))+
geom_hline(yintercept=1,col="red", linetype="longdash")+
geom_pointrange(aes(ymin=lower95,ymax=higher95,color=Site,shape=year),alpha=0.8,
	position = position_dodge2(width = 0.7))+
#  geom_line(aes(color = Site, group = Site), 
#            position = position_dodge2(width = 0.7))+
scale_x_discrete(labels=function(l) parse(text=l))+
scale_color_manual(values=c("#ff9211","#6e86e7"))+
theme_bw(base_size=16)+
theme(legend.position="bottom")+
facet_grid(.~VAR)+
xlab(NULL)+ylab(NULL)

lambsPlot
```

# 8. Applying demographic tools: elasticity and LTRE

## 8.1. Elasticities of mean demographic parameters

```{r, result="asis"}

# Mean elasticity for Parapiqueria
elasticity(mean(c(mpm_list_recruit_S11C,mpm_list_recruit_S11B)))

# Elasticities in Site S11B
elasticity(mean(mpm_list_recruit_S11B))

# Elasticities in Site S11C
elasticity(mean(mpm_list_recruit_S11C))
```

## 8.2. Life Table Response Experiment (LTRE)

```{r}
LTRE(mean(mpm_list_recruit_S11B), mean(mpm_list_recruit_S11C))
```

## 8.2. Life table response by year

```{r}

LTREyear<-list(
  #2022
LTRE(
mean(mpm_list_recruit_S11B[grep("2022$", names(mpm_list_recruit_S11B), value = TRUE)]),
mean(mpm_list_recruit_S11C[grep("2022$", names(mpm_list_recruit_S11C), value = TRUE)])),

#2023
LTRE(
mean(mpm_list_recruit_S11B[grep("2023$", names(mpm_list_recruit_S11B), value = TRUE)]),
mean(mpm_list_recruit_S11C[grep("2023$", names(mpm_list_recruit_S11C), value = TRUE)])),

#2024
LTRE(
mean(mpm_list_recruit_S11B[grep("2024$", names(mpm_list_recruit_S11B), value = TRUE)]),
mean(mpm_list_recruit_S11C[grep("2024$", names(mpm_list_recruit_S11C), value = TRUE)]))
)

#Mean
LTREyear%>%mean()

#Variance
LTREyear%>%popbio::var2()

```

# 9. Extinction risk

Three steps must be performed first:

1\. Define the initial number of individuals based on the mean value of immatures and reproductives

2 Simulate vital rates to include in the model

3\. Create a set of matrices with these new estimated variables

## 9.1. Define the initial number of individuals

```{r}
Mean_indiv<-Census_all_max%>%
  #filter(  Remove 1% dos dados para evitar outliers
  #  value < quantile(value,.995) & 
  #  value > quantile(value,.005))%>%
  group_by(stage)%>%
  summarise(Mean=mean(value),
            Median=median(value),
            sd=sd(value))

Mean_indiv
```

## 9.2. Simulate the vital rates

-   Simulate binomial distribution values from recruitment based on the best model

-   Simulate Survival, growth and stasis based on a beta distribution using beta function in `popbio` package

```{r}
BestModel_coef<-(BestModel%>%summary%>%coef)$cond%>%exp()


#BestModel_coef[1,1]  # Check best model coeficients
#BestModel_coef[1,2]

# Simulating recruitment
rec_sim<-
  0.1+    #A small factor has been added to make sure individuals will not recruit zero indivíduals
  rnbinom(1000, 
          mu =   BestModel_coef[1,1],
          size = sigma(BestModel) #Desviation in the model
  )


# Simulating U matrices values
mean_vrs_period<-mean(mpm_list)
var_vrs_period<-var2(mpm_list)

s1_sim<-replicate(1000,betaval(mean_vrs_period[1],var_vrs_period[1]))
s2_sim<-replicate(1000,betaval(mean_vrs_period[2],var_vrs_period[2]))
s3_sim<-replicate(1000,betaval(mean_vrs_period[4],var_vrs_period[4]))
```

### 9.2.1. Visual validation of simulated parameters

```{r, result="hold"}
par(mfrow=c(2,2))
  s1_sim%>%hist(main="stasis")
  abline(v=mean_vrs_period[1],col="red")
  
  s2_sim%>%hist(main="growth")
  abline(v=mean_vrs_period[2],col="red")
  
  s3_sim%>%hist(main="survival")
  abline(v=mean_vrs_period[2],col="red")
  
  hist(rec_sim,breaks=30,main="Recrutamento",freq=F)  #Simulated recruitment
  (RepIm$t1/RepIm$t0)%>%hist(breaks=30,freq=F,add=T,col=alpha("orange",0.6))  #Observed recruitment
  abline(v=exp(parameters::parameters(BestModel)[1,2]),col="red") #Esimated recruitment

```

## 9.3. Produce simulated MPMs

-   Based on the simulated vital rates produced, we can now produce the simulated MPMs.

Define de stages and the MPM format

```{r}

stages<-c("seed","Juvenile","Adult")

lifecycle_para <- expression(matrix2(c(
  s1,  recruit,
  s2, s3  ),   stages[-1] ))

vrstoch<-function(){
  vr <- list( s1=sample(s1_sim)[1],
              s2=sample(s2_sim)[1],
              s3=sample(s3_sim)[1],
              recruit=sample(rec_sim)[1])
  return(vr)
}

#Check if MPM is being built

eval(lifecycle_para, vrstoch())

```

### 9.3.1 Simulate several MPMs

```{r}
#Create the matrices
StochA_sim<-replicate(10000,(eval(lifecycle_para, vrstoch())))

StochA <- lapply(seq(dim(StochA_sim)[3]), function(i) StochA_sim[,,i])

StochA[[1]]


# Mean lambda
lapply(StochA,lambda)%>%unlist%>%mean


# Lambda range
lapply(StochA,lambda)%>%unlist%>%range

```

### 9.3.2. Check values

Check lambda

```{r, code_folding="hide", result="hold"}
par(mfrow=c(2,2))

# Simulated data
hist(unlist(lapply(StochA, lambda)),
     freq = FALSE,ylim=c(0,3),
     breaks=c(0, 0.33, 0.70, 1.00, 1.31, 1.66, 2.00, 2.31, 2.65, 3.2),
     main="Simulated data", xlab="Population growth rate (lambda)")

#S11B
hist(unlist(lapply(mpm_list_recruit_S11B, lambda)),
     freq = FALSE,ylim=c(0,3),col=alpha("#ff9211", 0.2),
     breaks=c(0, 0.33, 0.70, 1.00, 1.31, 1.66, 2.00, 2.31, 2.65,  3.2),
     main="S11B",xlab="Population growth rate (lambda)")
#S11C

hist(unlist(lapply(mpm_list_recruit_S11C, lambda)),
     freq = FALSE,ylim=c(0,3),col=alpha("#6e86e7", 0.2),
     breaks=c(0, 0.33, 0.70, 1.00, 1.31, 1.66, 2.00, 2.31, 2.65,  3.2),
     main="S11C",xlab="Population growth rate (lambda)")

# All
hist(unlist(lapply(StochA, lambda)),
     freq = FALSE,ylim=c(0,3),
     breaks=c(0, 0.33, 0.70, 1.00, 1.31, 1.66, 2.00, 2.31, 2.65,  3.2),
     main="ALL",xlab="Population growth rate (lambda)")

hist(unlist(lapply(mpm_list_recruit_S11B, lambda)),
     freq = FALSE,add=T,col=alpha("#ff9211", 0.2),
     breaks=c(0, 0.33, 0.70, 1.00, 1.31, 1.66, 2.00, 2.31, 2.65,  3.2))

hist(unlist(lapply(mpm_list_recruit_S11C, lambda)),
     freq = FALSE,add=T,col=alpha("#6e86e7", 0.2),
     breaks=c(0, 0.33, 0.70, 1.00, 1.31, 1.66, 2.00, 2.31, 2.65,  3.2))

```

## 9.4. Extinction growth rate 

```{r}
resultado<-stoch.quasi.ext(StochA, 
              n0 = Mean_indiv$Median[1:2], 
              Nx=Mean_indiv[3,3]/10,
              maxruns=10,
              tmax = 10, nreps=100)
resultado

matplot(resultado, xlab="Years", ylab="Cumulative extinction risk (%)", 
        type='l', lty=1, col="grey20", las=1,
        main="Extinction risk \n (cross extinction threshold of 10% initial population")

```

## 9.5 Simulating different scenarios

Create a function to automatise the vital rates reduction. This function is a adaptation of `stoch.quasi.ext` from `popbio` package.

This adaptation includes the parameter "damage" with a vector informing scenarios of vital rates reduction. Note all vital rates are impacted together.

```{r}
Myviab_func<-function (matrices, n0, Nx, tmax = 50, maxruns = 10, nreps = 5000, 
                       prob = NULL, sumweight = NULL, verbose = TRUE,damage=damage) {
  damage=damage
  if (is.list(matrices)) {
    matrices <- matrix(unlist(matrices), ncol = length(matrices))
  }
  x <- length(n0)
  if (is.null(sumweight)) {
    sumweight <- rep(1, x)
  }
  y <- dim(matrices)[2]
  ext <- matrix(numeric(maxruns * tmax), ncol = maxruns)
  for (h in 1:maxruns) {
    if (verbose) {
      message("Calculating extinction probability for run ", 
              h)
    }
    prob.ext <- numeric(tmax)
    for (i in 1:nreps) {
      n <- n0
      for (t in 1:tmax) {
        col <- sample(1:y, 1, prob = prob)
        A <- matrix(matrices[, col], nrow = x)
        n <- ((A-(A*damage)) %*% n)
        N <- sum(sumweight * round(n))
        if (N < Nx) {
          prob.ext[t] <- prob.ext[t] + 1
          break
        }
      }
    }
    prob.ext <- cumsum(prob.ext/nreps)
    ext[, h] <- prob.ext
  }
  ext
}
```

-   Define de impacts. a sequence of impact from 0% to 5% reduction in vital rates

```{r}
sim_damage<-seq(0,0.5,by=0.05)
sim_damage

```

```{r}

temp<-NULL
out<-NULL
tmax=11   #Years simulated
for(i in 1:length(sim_damage)){
temp<-Myviab_func(StochA, 
           n0 = Mean_indiv$Median[1:2], 
           Nx=Mean_indiv[3,3]/10,        #Mean_indiv[3,3]/10 representa 10% dos indivíduos médios
           maxruns=10,                   # Number of replications on each simulation
           tmax = tmax, nreps=1000,
           damage=sim_damage[i])
out[[i]]<-data.frame(MEAN=apply(temp,1,mean),
                     SD=apply(temp,1,sd),
                     Dano=sim_damage[i],
                     t=seq(1,tmax,by=1))
print(paste0("Performing simulation at damage ",sim_damage[i]*100,"%" ))
}

```

```{r}
out[[1]]
```

```{r}
viab_para<-do.call(rbind,out)%>%
  mutate(PROP=100-(MEAN*100),
         ymin=100-((MEAN-SD)*100),
         ymax=100-((MEAN+SD)*100))

viab_para
```

# Glossary

C

:   C: is a matrix somehow
